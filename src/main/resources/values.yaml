# Default values for kube-prometheus-stack.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

## Provide a name in place of kube-prometheus-stack for `app:` labels
##

nameOverride: ""

## Override the deployment namespace
##
namespaceOverride: ""

## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
##
kubeTargetVersionOverride: ""

## Allow kubeVersion to be overridden while creating the ingress
##
kubeVersionOverride: ""

## Provide a name to substitute for the full names of resources
##
fullnameOverride: ""

## Labels to apply to all resources
##
commonLabels: 
  monitored: dox-prometheus



## Create default rules for monitoring the cluster
##
defaultRules:
  create: true
  rules:
    alertmanager: true
    couchbase: true
    nativeCouchbase: true
    couchbaseNonCritical: false
    elasticsearch: true
    elasticsearchNonCritical: false
    kafka: true
    kafkaMSK: false
    kafkaAnomaly: true
    kafkaAnomalyAWS: false
    kafkaNonCritical: false
    kafkaProducerConsumer: true
    postgres: true
    postgresNonCritical: false
    springboot: true
    springbootNonCritical: false
    etcd: true
    general: true
    k8s: false
    kubeApiserver: true
    kubeApiserverAvailability: true
    kubeApiserverError: true
    kubeApiserverSlos: true
    kubelet: true
    kubePrometheusGeneral: true
    kubePrometheusNodeAlerting: true
    kubePrometheusNodeRecording: true
    kubernetesAbsent: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeScheduler: true
    kubeStateMetrics: true
    network: true
    node: true
    prometheus: true
    prometheusOperator: true
    time: true

  ## Runbook url prefix for default rules
  runbookUrl: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#
  ## Reduce app namespace alert scope
  appNamespacesTarget: ".*"

  ## Labels for default rules
  labels: {}
  ## Annotations for default rules
  annotations: {}

  ## Additional labels for PrometheusRule alerts
  additionalRuleLabels: {}

additionalPrometheusRulesMap: 
  example:
    groups:
    - name: general.rules
      rules:
      - alert: exampleAlert
        annotations:
          message: '{{`{{ $value }}`}}% of the {{`{{ $labels.job }}`}} targets are down.'
        expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) > 10
        for: 10m
        labels:
          severity: warning

## MS360 Rules Parameters
threshold:              # PushGateway Job failure Threshold
  jobPercentage: 50

couchbaseParam:
  couchbaseTooFragmented: 30
  couchbaseRebalanceStarted:  5m
  couchbaseRebalanceSuccess:  5m
  couchbaseQuotaUsageHigh:  85
  couchbaseHardOutOfMemoryErrors:  5m
  couchbaseDiskUsageIncreasing:  1E+12
  couchbaseDiskUsageDecreasing:  1E+12
  couchbaseHighCacheMissRate: 20
  couchbaseHighDiskWriteQueue: 1000000
  enableCouchbaseDownAlert: true
  enableCouchbaseNotBalancedAlert: true
  enableCouchbaseNodeNotHealthyAlert: true
  enableCouchbaseQuotaUsageHighAlert: true
  enableCouchbaseHardOutOfMemoryErrorsAlert: true

elasticsearchParam:
  elasticsearchTooFewNodesRunning: 3
  elasticsearchHeapTooHigh: 0.9
  elasticsearchHighCPUUsageWarning: 85
  elasticsearchHighCPUUsageCritical: 90
  elasticsearchNumberOfPendingTasksIncreasing: 5m
  enableElasticsearchDownAlert: true
  enableElasticsearchTooFewNodesRunningAlert: true
  enableElasticsearchHeapTooHighAlert: true
  enableElasticsearchHighCPUUsageCriticalAlert: true
  enableElasticsearchClusterHealthStatusAlert: true

awsESParam:
  enableESStatusRedAlert: true
  enableESAutomatedSnapshotFailureAlert: true
  enableESclusterindexwritesblockAlert: true
  enableESMasterJVMPressureAlert: true
  enableESDataJVMPressureAlert: true
  enableESDataCPUUsageCriticalAlert: true
  enableESMasterCPUUsageCriticalAlert: true
  ESClusterRedStatus: 0
  ESAutomatedSnapshotFailure: 0
  ESClusterIndexWritesBlocked: 0
  ESMasterNodeJVMmemoryPressure: 80
  ESDataNodeJVMmemoryPressure: 80
  ESDataHighCPUUsage: 90
  ESMasterHighCPUUsage: 90

kafkaParam:
  kafkaHighBytesIn5k: 5000
  kafkaHighBytesIn10k: 10000
  kafkaHighBytesIn15k: 15000
  kafkaJVMHighMemoryWarning: 0.85
  kafkaJVMHighmemoryCritical: 0.9
  kafkaHighHeapMemoryWarning: 0.85
  kafkaHighHeapMemoryCritical: 0.9
  kafkaIOThreadsIdleTime: 5m
  kafkaIOThreadsIdle: 30
  kafkaNetworkThreadsIdle: 30
  kafkaLagIncreasingTime: 60m
  kafkaLagIncreasing: 0
  enableKafkaIsDownAlert: true
  enableKafkaHighBytesIn10kAlert: true
  enableKafKafkaHighBytesIn15kAlert: true
  enableKafkaJVMHighmemoryCriticalAlert: true
  enableKafkaHighHeapMemoryCriticalAlert: true
  enableKafkaRedMonitor: true

kafkaAnomalyParam:
  enableKafkaProducerAuthenticationFailuresAlert: true
  kafkaProducerAuthenticationFailures: 1
  kafkaProducerAuthenticationAlertDurationFor: 300s
  enableKafkaProducerRequestsErrorsAlert: true
  kafkaProducerRequestsErrors: 1
  kafkaProducerRequestsAlertDurationFor: 300s
  enableKafkaProducerRecordsErrorsAlert: true
  kafkaProducerRecordsErrors: 1
  kafkaProducerRecordsAlertDurationFor: 300s
  enableKafkaConsumerAuthenticationFailuresAlert : true
  kafkaConsumerAuthenticationFailures: 1
  kafkaConsumersAuthenticationAlertDurationFor: 300s
  enableKafkaFetchRequestsErrorsAlert: true
  kafkaFetchRequestsErrors: 1
  kafkaFetchRequestsAlertDurationFor: 300s
  enableKafkaReplicasFetcherMetricsAuthenticationFailuresAlert: true
  kafkaReplicasFetcherMetricsAuthenticationFailures: 1
  kafkaReplicaFetcherMetricsAuthenticationAlertDurationFor: 300s

MSKkafkaParam:
  kafkaHighBytesIn10k: 10000
  kafkaHighBytesIn15k: 15000
  kafkaHighHeapMemoryCritical: 0.9
  kafkaDialerNetConn: 0
  enableKafkaIsDownAlert: true
  enableKafkaHighBytesIn10kAlert: true
  enableKafKafkaHighBytesIn15kAlert: true
  enableKafkaDialerNetConnAlert: true
  enableKafkaHighHeapMemoryCriticalAlert: true

kafkaProducerParam:
  KafkaProducerAverageRequestLatencyHigh:
    latencyThreshold: 1
    forDuration: 1m
  KafkaProducerAverageIOWaitTimeHigh:
    avgIOWaitTimeThreshold: 27012434631
    forDuration: 1m
  KafkaProducerActiveConnectionCountLow:
    forDuration: 1m
  KafkaProducerAverageRecordSizeHigh:
    avgRecordSizeThreshold: 1
    forDuration: 1m
  enableKafkaProducerAverageRequestLatencyHigh: true
  enableKafkaProducerAverageIOWaitTimeHigh: true
  enableKafkaProducerActiveConnectionCountLow: true
  enableKafkaProducerAverageRecordSizeHigh: true

kafkaConsumerParam:
  KafkaConsumerAverageFetchLatencyHigh:
    latencyThreshold: 501
    forDuration: 1m
  KafkaConsumerAverageIOWaitTimeHigh:
    avgIOWaitTimeThreshold: 187605190
    forDuration: 1m
  KafkaConsumerActiveConnectionCountLow:
    forDuration: 1m
  enableKafkaConsumerAverageFetchLatencyHigh: true
  enableKafkaConsumerAverageIOWaitTimeHigh: true
  enableKafkaConsumerActiveConnectionCountLow: true

postgresParam:
  PGConnPercWarning: 75
  PGConnPercCritical: 90
  PGDBSizeWarning: 1.073741824e+11
  PGDBSizeCritical: 2.68435456e+11
  PGReplicationLagWarning: 300
  PGReplicationLagCritical: 600
  pg_stat_activity_max_tx_duration_idle: 900
  pg_stat_activity_max_tx_duration_query: 86400
  enablePGSqlDownAlert: true
  enablePGExporterScrapeErrorAlert: true
  enablePGIdleTxnWarningCriticalAlert: true
  enablePGQueryTimeCriticalAlert: true
  enablePGDBSizeCriticalAlert: true
  enablePGReplicationLagCriticalAlert: true
  enablePGConnPercCriticalAlert: true

RDSpostgresParam:
  PGConnAvgCritical: 90
  PGReplicationLagCritical: 600
  PGCPUAvgCritical: 90
  enablePGSqlDownAlert: true
  enableCloudwatchExporterScrapeErrorAlert: true
  enablePGReplicationLagCriticalAlert: true
  enablePGConnectionAvgCriticalAlert: true
  enablePGCPUAvgCriticalAlert: true

springBootParam:
  springBootCpuUsageInfo: 50
  springBootCpuUsageWarning: 80
  springBootCpuUsageCritical: 90
  SpringBootHeapSpaceUsageInfo: 50
  SpringBootHeapSpaceUsageWarning: 80
  springBootHeapSpaceUsageCritical: 90
  springBootGcTimeInfo: .3
  springBootGcTimeWarning: .5
  springBootGcTimeCritical: .8
  springBootOpenFilesInfo: 500
  springBootOpenFilesWarning: 300
  springBootOpenFilesCritical: 100
  enableSpringBootDownAlert: true
  enableSpringBootCpuUsageCriticalAlert: true
  enableSpringBootHeapSpaceUsageCriticalAlert: true
  enableSpringBootGcTimeCriticalAlert: true
  enableSpringBootOpenFilesCriticalAlert: true

## Additional Application Service Monitors
servicemetrics:
  schema:
    http: true
    https: true
    


## If true federation with the cluster scoped prometheus will be created
federation: 
  enabled: false
  federated_from: openshift-monitoring

  #metricsRegex: "namespace1|namespace2"     ### Note: If you want to get kubernetes resource metrics from multiple namespaces. for all namespace user regex: ".*" 

# Configuration for AWS App Mesh and Azure Open Service Mesh envoy proxy monitoring
enableEnvoyProxyMonitoring: false

# Configuration for Azure Open Service Mesh controlplane monitoring
enableControlPlaneMonitoring: false

appServiceMonitors:
  appServiceMonitor:
    interval: 25s
    scrapeTimeout: 20s
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_label_monitored ]
        separator: ;
        regex: __meta_kubernetes_pod_label_(.+)
        targetLabel: podlabel
        replacement: $1
        action: labelmap

    metricRelabelings: []

  appServiceMonitorHttp:
    interval: 25s
    scrapeTimeout: 20s
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_label_monitored ]
        separator: ;
        regex: __meta_kubernetes_pod_label_(.+)
        targetLabel: podlabel
        replacement: $1
        action: labelmap

    metricRelabelings: []

  appServiceMonitorHttps:
    interval: 25s
    scrapeTimeout: 20s
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_label_monitored ]
        separator: ;
        regex: __meta_kubernetes_pod_label_(.+)
        targetLabel: podlabel
        replacement: $1
        action: labelmap

    metricRelabelings: []

  appServiceMonitorPrometheusHttp:
    interval: 25s
    scrapeTimeout: 20s
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_label_monitored ]
        separator: ;
        regex: __meta_kubernetes_pod_label_(.+)
        targetLabel: podlabel
        replacement: $1
        action: labelmap
    #actuatorrelabeling: []
    #metrelabeling: []

    metricRelabelings: []

  appServiceMonitorPrometheusHttps:
    interval: 25s
    scrapeTimeout: 20s
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_label_monitored ]
        separator: ;
        regex: __meta_kubernetes_pod_label_(.+)
        targetLabel: podlabel
        replacement: $1
        action: labelmap
      - action: drop
        regex: (.*broker)
        sourceLabels:
        - __meta_kubernetes_service_label_app
    #actuatorrelabeling:
    #  - action: drop
    #    regex: (.*cbkc-plugin*)
    #    sourceLabels:
    #    - __meta_kubernetes_service_label_app
    #metrelabeling: []

    metricRelabelings: []

  federationMetrics:
    interval: 25s
    scrapeTimeout: 20s
    relabelings:
      - sourceLabels: [ __meta_kubernetes_pod_label_monitored ]
        separator: ;
        regex: __meta_kubernetes_pod_label_(.+)
        targetLabel: podlabel
        replacement: $1
        action: labelmap

    metricRelabelings: []

## if jmx exporter running inside K8s, can use serviceMonitor, otherwise, use additionalScrapeConfigs
kafkajmx:
  serviceMonitor:
    enabled: false
    job_name: kafka-jmx-servicemonitor-metrics
    metrics_path: /metrics
    port: jmx-exporter
    scheme: http
    interval: 25s
    scrapeTimeout: 20s
    appLabel: kafka
    relabelings: []
    metricRelabelings: []

## if kafka exporter running inside K8s, can use serviceMonitor, otherwise, use additionalScrapeConfigs
kafka:
  serviceMonitor:
    enabled: false
    job_name: kafka-servicemonitor-metrics
    metrics_path: /metrics
    port: kafka-exporter
    scheme: http
    interval: 25s
    scrapeTimeout: 20s
    appLabel: kafka-exporter
    relabelings: []
    metricRelabelings: []

# if cb exporter running inside K8s, can use serviceMonitor, otherwise, use additionalScrapeConfigs
couchbase:
  serviceMonitor:
    enabled: false
    job_name: couchbase-servicemonitor-metrics
    metrics_path: /metrics
    port: http
    scheme: http
    interval: 25s
    scrapeTimeout: 20s
    appLabel: prometheus-couchbase-exporter
    relabelings: []
    metricRelabelings: []

## if es exporter running inside K8s, can use serviceMonitor, otherwise, use additionalScrapeConfigs
elasticsearch:
  serviceMonitor:
    enabled: false
    job_name: elasticsearch-servicemonitor-metrics
    metrics_path: /metrics
    port: http
    scheme: http
    interval: 25s
    scrapeTimeout: 20s
    appLabel: elasticsearch-exporter
    relabelings: []
    metricRelabelings: []

## if postgres exporter running inside K8s, can use serviceMonitor, otherwise, use additionalScrapeConfigs
postgresql:
  serviceMonitor:
    enabled: false
    job_name: postgresql-servicemonitor-metrics
    metrics_path: /metrics
    port: http
    scheme: http
    interval: 25s
    scrapeTimeout: 20s
    appLabel: prometheus-postgres-exporter
    relabelings: []
    metricRelabelings: []

## if cloudwatch exporter running inside K8s, can use serviceMonitor, otherwise, use additionalScrapeConfigs
cloudwatch:
  serviceMonitor:
    enabled: false
    job_name: cloudwatch-servicemonitor-metrics
    metrics_path: /metrics
    port: http
    scheme: http
    interval: 1m
    scrapeTimeout: 30s
    appLabel: prometheus-cloudwatch-exporter
    relabelings: []
    metricRelabelings: []

## Configuration for aws cognito integration
cognito:
  enabled: false
  # userPoolClientId: "1p5tcebeg4esh4e5r0qm9ghijl"
  # userPoolClientSecret: "1cm8hanm6k4mfcmpdgdt8r9noa1ce9u985cdc4u82r06aq9nrlvq"
  # userPoolDomain: "https://amdocs-observers.auth.eu-west-1.amazoncognito.com"
  # userPoolRegion: "eu-west-1"
  # userPoolId: "eu-west-1_2QR5FCmpp"
  # oauthProxyImage: "quay.io/pusher/oauth2_proxy:v4.0.0"
  # # To generate a strong cookie secret use the command below:
  # # python -c 'import os,base64; print base64.urlsafe_b64encode(os.urandom(16))'
  # oauthProxyCookieSecret: "YkAEanDHd2v+YNCcBSGsiQ=="

# If ssl enabled, the enable proxy as well
proxy: false   
proxyEnv:
  http_proxy: http://10.65.1.6:8080
  https_proxy: http://10.65.1.6:8080
  HTTPS_PROXY: http://10.65.1.6:8080
  HTTP_PROXY: http://10.65.1.6:8080
  no_proxy: eu-west-1.es.amazonaws.com,es.amazonaws.com,.eks.amazonaws.com,10.100.0.0/16,172.20.0.0/16,localhost,127.0.0.1,100.65.64.0/21,169.254.169.254,.internal,.corp.amdocs.com,ilnexusdmz02,ilnexusdmz02.corp.amdocs.com,10.238.7.2,ilcapslf.corp.amdocs.com,ilnexusdmz01,monitoring.eu-west-1.amazonaws.com,ms360-monitoring-prometheus,ms360-monitoring-alertmanager,ms360-monitoring-prometheus-internal,ms360-monitoring-alertmanager-internal
  NO_PROXY: eu-west-1.es.amazonaws.com,es.amazonaws.com,.eks.amazonaws.com,10.100.0.0/16,172.20.0.0/16,localhost,127.0.0.1,100.65.64.0/21,169.254.169.254,.internal,.corp.amdocs.com,ilnexusdmz02,ilnexusdmz02.corp.amdocs.com,10.238.7.2,ilcapslf.corp.amdocs.com,ilnexusdmz01,monitoring.eu-west-1.amazonaws.com,ms360-monitoring-prometheus,ms360-monitoring-alertmanager,ms360-monitoring-prometheus-internal,ms360-monitoring-alertmanager-internal

# If enabled, a nginx sidecar will be added to the prometheus pod in order to support ssl
ssl:
  enabled: false
  http_redirect_port: 80
  nginxImage: ilnexusdmz02.corp.amdocs.com:8084/nginx:1.21.1
  # certificate: |
  #   -----BEGIN CERTIFICATE-----
  #   MIIDVzCCAj+gAwIBAgIJAOwjzDrnnWD3MA0GCSqGSIb3DQEBCwUAMEIxCzAJBgNV
  #   BAYTAlhYMRUwEwYDVQQHDAxEZWZhdWx0IENpdHkxHDAaBgNVBAoME0RlZmF1bHQg
  #   Q29tcGFueSBMdGQwHhcNMTkwNzAyMTIwMDE4WhcNMjkwNjI5MTIwMDE4WjBCMQsw
  #   CQYDVQQGEwJYWDEVMBMGA1UEBwwMRGVmYXVsdCBDaXR5MRwwGgYDVQQKDBNEZWZh
  #   dWx0IENvbXBhbnkgTHRkMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
  #   3YeIEWh0S+D4w8zFFlhvNIEl1pvbU1WVjAXLVeQA84ESRFSIGfvGn3K3AFozH41n
  #   u6W6AnEcGSVH2RzZ8GI6EIq7c5XhHcar0YtsfsFV/mgHf7q1xru/B7xzH92OGF+e
  #   gPVhZn5xhnjLK3MhrHwRFvcANjK3CTVtR6pQHX6WUjfDkP7215DhiSyE6alRa3UA
  #   zdwrDK8MmuJKeZGpmoKiCMVy/7MFOAWJZH4tWNfalYQ/xfEe4NUI31VLmU5BqNxM
  #   d30Udem+5ofPvQyTPNRS6rrC4eo9TZd2aIIfXf8v+UgyprKumJHUUeaCnCfHVvSo
  #   UmjUBdABUwWpMyN7NavedwIDAQABo1AwTjAdBgNVHQ4EFgQUabUt1S62EoSSIFG4
  #   c9X686CxCmMwHwYDVR0jBBgwFoAUabUt1S62EoSSIFG4c9X686CxCmMwDAYDVR0T
  #   BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEANYLfkvYGXQ0Jx0f+0/7EFOFQBWAG
  #   IAg0XDG6IZA7buq4KoDh0bkn5FJN6CZTqwvWhw0b1PGZR0rPhFk+ftGGGipWJup+
  #   j2j2FyH2r/yTg84Qj/UM9/l9OL3WXlgYL6S1z6swqRaLBPeVywHrrIKQf1kq7q7q
  #   AqnGj9b7Kua8pzg4OuqFvt7n/RBUgAjvMbhx7mgVmlZo7U+vAtXHbOMdHXsKjjq+
  #   FlEogScjLLkLBP7KsiirXvX6KJFSAs8hxCMWe/1OKLbWpXI0StxcepWNKPjZ+IQn
  #   s2Tbh3lufNHNfr+AnoUfjlictGgdB+6YUDHn4v4LgyKJWWBSRGkKhrUv8g==
  #   -----END CERTIFICATE-----
  # key: |
  #   -----BEGIN PRIVATE KEY-----
  #   MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDdh4gRaHRL4PjD
  #   zMUWWG80gSXWm9tTVZWMBctV5ADzgRJEVIgZ+8afcrcAWjMfjWe7pboCcRwZJUfZ
  #   HNnwYjoQirtzleEdxqvRi2x+wVX+aAd/urXGu78HvHMf3Y4YX56A9WFmfnGGeMsr
  #   cyGsfBEW9wA2MrcJNW1HqlAdfpZSN8OQ/vbXkOGJLITpqVFrdQDN3CsMrwya4kp5
  #   kamagqIIxXL/swU4BYlkfi1Y19qVhD/F8R7g1QjfVUuZTkGo3Ex3fRR16b7mh8+9
  #   DJM81FLqusLh6j1Nl3Zogh9d/y/5SDKmsq6YkdRR5oKcJ8dW9KhSaNQF0AFTBakz
  #   I3s1q953AgMBAAECggEBAJBlbV3bdiKXi88kxDqifin9HIrcoIObP5U7maRsr+oI
  #   8b00VFqLcJYyEfLa2IPyifEYPzykOapyUHjGg5Balnp9012uDkR4YTp+5MNihgkE
  #   Cg9Jxn65lfUW8vuMULi2avyn9Ur3lEwQpG235/UkEYekjLDHMeOAvbB+5SW0g4sm
  #   szQ3rMpf261HLBikhbfVFHOdxYjK1+/EaJxq/GZUvux9RIBoBBDEQ6QBW3AHjK+C
  #   uc/XfZ+IeJfA8rqwIWbsEFVGur/WPT+hhhNh05cn2HZhwLdRutOWunVqRaSkxCXP
  #   uKADaw+G0RrlNTsXkc69AGcSM0N634dCI42Dh9L/zaECgYEA+erL6D2E5/ixfTgL
  #   pLo1vUVQOzD4scNzh3X8l3avdBZH9gak0Udh+ekB1O2GRAqAAovaMPhlp1Cbrt9f
  #   jeHijjim7NbpH2WFT3pQAnugVNjrZenQxb7+3jifeFLKXL/K3SoxDMBXQF8TDvgg
  #   2VAaVB9TYecxHaf2L1OorDblQmkCgYEA4uvat59BsaTjDoCy9XRnTlD0x6e9lvtE
  #   eXEJe12Ndr2AM/VrtiicDhiuv4LH0k8LU6L0oTrlMtYdKVr0G3I6jbtWHNe5aQSt
  #   wLOkLlK/U7262WizinA6Lwx2rhkJBmlZSmnL77WUSUSjPrXqr7Y+tpnI2x1+Qi+J
  #   //IcXb2EPd8CgYA3/Ov1eWK+/S32GYoyOy0y28ZfnFb6rAjDviD6ZbJk1WEFsvNV
  #   lyJR9Gteol/D+n43E9GrytorI5ndEVm81XAF4Jc00+iiCgJVjddC1yNSHs6NpFYL
  #   FwDOE+1V7WY8ZRMzfPJEIwro9uO0JM5nINTlJ6dsdXjBZhF3BMzy19GAkQKBgF3e
  #   Pr2SjXRUxvAq4NyUaF+HMZqdktmv2yV5UvjLPEeQIEZeEU5rAhL6tt7V1J70v7S7
  #   RgKkAEfFNo7y5uOrnvkeM4L+4d3FZ7K9HP9kj2yuoWpp20s7HXZ062sdRBR448za
  #   ctgX/t1IHVdFnW1OMCcM+n9Zde8fBzDvEDjeuqN1AoGAU/Qkkh7VYM6UzYrxi5W3
  #   rgexCMLaLTFg03GWDUA8VzPZRjFo4LxUQBdfPwP06V47mm5L8c2k3ycmUL0EPFY0
  #   JLXTQKoEoH0c9eLGDj9zk5Bj0oX4XwW6hJdwItArxZAANxdUyOsmzDJA5YrxB9ZN
  #   AzibW47Sv99Gm6p31amlVy4=
  #   -----END PRIVATE KEY-----
  # nginxConfig: |-
  #   user              root root;
  #   worker_processes  auto;
  #   error_log         "/opt/bitnami/nginx/logs/error.log" debug;
  #   pid               "/opt/bitnami/nginx/tmp/nginx.pid";
  #   events {
  #       worker_connections  1024;
  #   }
  #   http {
  #       include       mime.types;
  #       default_type  application/octet-stream;
  #       log_format    main '$remote_addr - $remote_user [$time_local] '
  #                         '"$request" $status  $body_bytes_sent "$http_referer" '
  #                         '"$http_user_agent" "$http_x_forwarded_for"';
  #       access_log    "/opt/bitnami/nginx/logs/access.log";
  #       add_header    X-Frame-Options SAMEORIGIN;

  #       client_body_temp_path  "/opt/bitnami/nginx/tmp/client_body" 1 2;
  #       proxy_temp_path        "/opt/bitnami/nginx/tmp/proxy" 1 2;
  #       fastcgi_temp_path      "/opt/bitnami/nginx/tmp/fastcgi" 1 2;
  #       scgi_temp_path         "/opt/bitnami/nginx/tmp/scgi" 1 2;
  #       uwsgi_temp_path        "/opt/bitnami/nginx/tmp/uwsgi" 1 2;

  #       sendfile           on;
  #       tcp_nopush         on;
  #       tcp_nodelay        off;
  #       gzip               on;
  #       gzip_http_version  1.0;
  #       gzip_comp_level    2;
  #       gzip_proxied       any;
  #       gzip_types         text/plain text/css application/javascript text/xml application/xml+rss;
  #       keepalive_timeout  65;
  #       ssl_protocols      TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;
  #       ssl_ciphers        HIGH:!aNULL:!MD5;
  #       client_max_body_size 80M;
  #       server_tokens off;

  #       include  "/opt/bitnami/nginx/conf/server_blocks/*.conf";
  #       server {
  #           listen              443 ssl;
  #           server_name         tls-sidecar;
  #           ssl_certificate     /app/cert/certificate.pem;
  #           ssl_certificate_key /app/cert/key.pem;

  #           location / {
  #               proxy_pass http://localhost:9090;
  #           }
  #       }
  #   }

## global values for all components
global:
  # Provide the platform on which the monitoring is to be deployed. The parameter is case-sensitive
  # Accepted values: OCP|AWS|AZURE
  platform: ""
  namespaceScoped: false
  rbac:
    create: false

  ## Reference to one or more secrets to be used when pulling images
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  imagePullSecrets: []
  # - name: "image-pull-secret"

## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
##
alertmanager:

  ## Deploy alertmanager
  ##
  enabled: true

  ## Annotations for Alertmanager
  ##
  annotations: {}

  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2
  ##
  apiVersion: v2

  ## Service account for Alertmanager to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    create: true
    name: ""
    annotations: {}

  ## Configure pod disruption budgets for Alertmanager
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  ## This configuration is immutable once created and will require the PDB to be deleted to be changed
  ## https://github.com/kubernetes/kubernetes/issues/45398
  ##
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  ##
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
      - match:
          alertname: Watchdog
        receiver: 'null'
    receivers:
    - name: 'null'
    templates:
    - '/etc/alertmanager/config/*.tmpl'

  ## Pass the Alertmanager configuration directives through Helm's templating
  ## engine. If the Alertmanager configuration contains Alertmanager templates,
  ## they'll need to be properly escaped so that they are not interpreted by
  ## Helm
  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function
  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string
  ##      https://prometheus.io/docs/alerting/notifications/
  ##      https://prometheus.io/docs/alerting/notification_examples/
  tplConfig: false

  ## Alertmanager template files to format alerts
  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if
  ## they have a .tmpl file suffix will be loaded. See config.templates above
  ## to change, add other suffixes. If adding other suffixes, be sure to update
  ## config.templates above to include those suffixes.
  ## ref: https://prometheus.io/docs/alerting/notifications/
  ##      https://prometheus.io/docs/alerting/notification_examples/
  ##
  templateFiles: {}
  #
  ## An example template:
  #   template_1.tmpl: |-
  #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
  #
  #       {{ define "slack.myorg.text" }}
  #       {{- $root := . -}}
  #       {{ range .Alerts }}
  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
  #         *Cluster:*  {{ template "cluster" $root }}
  #         *Description:* {{ .Annotations.description }}
  #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
  #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
  #         *Details:*
  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
  #           {{ end }}
  #       {{ end }}
  #       {{ end }}

  ingress:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}

    labels: {}

    ## Hosts must be provided if Ingress is enabled.
    ##
    hosts: []
      # - alertmanager.domain.com

    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## TLS configuration for Alertmanager Ingress
    ## Secret must be manually created in the namespace
    ##
    tls: []
    # - secretName: alertmanager-general-tls
    #   hosts:
    #   - alertmanager.example.com

  ## Configuration for Alertmanager secret
  ##
  secret:
    annotations: {}

  ## Configuration for creating an Ingress that will map to each Alertmanager replica service
  ## alertmanager.servicePerReplica must be enabled
  ##
  ingressPerReplica:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}
    labels: {}

    ## Final form of the hostname for each per replica ingress is
    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
    ##
    ## Prefix for the per replica ingress that will have `-$replicaNumber`
    ## appended to the end
    hostPrefix: ""
    ## Domain that will be used for the per replica ingress
    hostDomain: ""

    ## Paths to use for ingress rules
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## Secret name containing the TLS certificate for alertmanager per replica ingress
    ## Secret must be manually created in the namespace
    tlsSecretName: ""

    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
    ##
    tlsSecretPerReplica:
      enabled: false
      ## Final form of the secret for each per replica ingress is
      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
      ##
      prefix: "alertmanager"

  ## Configuration for Alertmanager service
  ##
  service:
    annotations: {}
    labels:
      app.kubernetes.io/part-of: infra-platform
      app.kubernetes.io/component: alertmanager
      app.amdocs.com/domain: monitoring
      app.amdocs.com/provider: amdocs
    clusterIP: ""

    ## Port for Alertmanager Service to listen on
    ##
    port: 9093
    ## To be used with a proxy extraContainer port
    ##
    targetPort: 9093
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
    nodePort: 30903
    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##

    ## Additional ports to open for Alertmanager service
    additionalPorts: []

    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    ## Service type
    ##
    type: ClusterIP

  ## Configuration for creating a separate Service for each statefulset Alertmanager replica
  ##
  servicePerReplica:
    enabled: false
    annotations: {}

    ## Port for Alertmanager Service per replica to listen on
    ##
    port: 9093

    ## To be used with a proxy extraContainer port
    targetPort: 9093

    ## Port to expose on each node
    ## Only used if servicePerReplica.type is 'NodePort'
    ##
    nodePort: 30904

    ## Loadbalancer source IP ranges
    ## Only used if servicePerReplica.type is "LoadBalancer"
    loadBalancerSourceRanges: []
    ## Service type
    ##
    type: ClusterIP

  ## If true, create a serviceMonitor for alertmanager
  ##
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    selfMonitor: true

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""

    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
    ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
    tlsConfig: {}

    bearerTokenFile:

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

  ## Settings affecting alertmanagerSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#alertmanagerspec
  ##
  alertmanagerSpec:
    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
    ##
    podMetadata:
      labels:
        app.kubernetes.io/part-of: infra-platform
        app.kubernetes.io/component: alertmanager
        app.amdocs.com/domain: monitoring
        app.amdocs.com/provider: amdocs

    ## Image of Alertmanager
    ##
    image:
      repository: docker-registry-proxy.corp.amdocs.com/prometheus/alertmanager
      tag: v0.25.0
      sha: ""

    ## If true then the user will be responsible to provide a secret with alertmanager configuration
    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
    ##
    useExistingSecret: false

    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
    ##
    secrets: []

    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
    ##
    configMaps: []

    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
    ##
    # configSecret:

    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
    ##
    alertmanagerConfigSelector: {}
    ## Example which selects all alertmanagerConfig resources
    ## with label "alertconfig" with values any of "example-config" or "example-config-2"
    # alertmanagerConfigSelector:
    #   matchExpressions:
    #     - key: alertconfig
    #       operator: In
    #       values:
    #         - example-config
    #         - example-config-2
    #
    ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
    # alertmanagerConfigSelector:
    #   matchLabels:
    #     role: example-config

    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
    ##
    alertmanagerConfigNamespaceSelector: {}
    ## Example which selects all namespaces
    ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
    # alertmanagerConfigNamespaceSelector:
    #   matchExpressions:
    #     - key: alertmanagerconfig
    #       operator: In
    #       values:
    #         - example-namespace
    #         - example-namespace-2

    ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
    # alertmanagerConfigNamespaceSelector:
    #   matchLabels:
    #     alertmanagerconfig: enabled

    ## Define Log Format
    # Use logfmt (default) or json logging
    logFormat: logfmt

    ## Log level for Alertmanager to be configured with.
    ##
    logLevel: info

    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
    ## running cluster equal to the expected size.
    replicas: 1

    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
    ##
    retention: 120h

    ## Storage is the definition of how storage will be used by the Alertmanager instances.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
    storage: {}
    # volumeClaimTemplate:
    #   spec:
    #     storageClassName: gluster
    #     accessModes: ["ReadWriteOnce"]
    #     resources:
    #       requests:
    #         storage: 50Gi
    #   selector: {}


    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name.stringfalse
    ##
    externalUrl:

    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
    ##
    routePrefix: /

    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
    ##
    paused: false

    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

    ## Define resources requests and limits for single Pods.
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
    # limits:
    #   cpu: 250m
    #   memory: 1Gi
    # requests:
    #   cpu: 50m
    #   memory: 1Gi

    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
    ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
    ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
    ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
    ##
    podAntiAffinity: "soft"

    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
    ##
    podAntiAffinityTopologyKey: kubernetes.io/hostname

    ## Assign custom affinity rules to the alertmanager instance
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    ##
    affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: kubernetes.io/e2e-az-name
    #         operator: In
    #         values:
    #         - e2e-az1
    #         - e2e-az2

    ## If specified, the pod's tolerations.
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"

    ## If specified, the pod's topology spread constraints.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    ##
    topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app: alertmanager

    ## SecurityContext holds pod-level security attributes and common container settings.
    ## This defaults to non root user with uid 1000 and gid 2000.*v1.PodSecurityContextfalse
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    ##
    # securityContext:
    #   runAsGroup: 2000
    #   runAsNonRoot: true
    #   runAsUser: 1000
    #   fsGroup: 2000

    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
    ## Note this is only for the Alertmanager UI, not the gossip communication.
    ##
    listenLocal: false

    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
    ##
    containers: []

    # Additional volumes on the output StatefulSet definition.
    volumes: []

    # Additional VolumeMounts on the output StatefulSet definition.
    volumeMounts: []

    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
    ## (permissions, dir tree) on mounted volumes before starting prometheus
    initContainers: []

    ## Priority class assigned to the Pods
    ##
    priorityClassName: ""

    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
    ##
    additionalPeers: []

    ## PortName to use for Alert Manager.
    ##
    portName: "web"

    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918
    ##
    clusterAdvertiseAddress: false

    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.
    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.
    forceEnableClusterMode: false


## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
##
grafana:
  image:
    repository: grafana/grafana-oss
    tag: 9.5.2
    sha: ""
    pullPolicy: IfNotPresent
  enabled: true
  namespaceOverride: ""
  ssl:
    enabled: false
    #dns: ms360-monitoring-grafana-K8S_PROJECT.K8S_HOST  # Enable when SSL enabled (DNS name of route / ingress)
  # readinessProbe:
  #   scheme: HTTPS  # Enable when SSL enabled
  # livenessProbe:
  #   scheme: HTTPS  # Enable when SSL enabled

  testFramework:
    enabled: false
  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
  ##
  forceDeployDatasources: false

  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled
  ##
  forceDeployDashboards: false
  # postgres database
  enablePersistencyWithPostgres: false

  grafanaPersistencyConfigs:
    # secret manager broker values
    secretManagerServiceBroker:
      resourceType:
      serviceClassName:
      servicePlanName:
      tags:
        team: observers
      parameters:
        secretManagerLogicalName:

    # Postgres Admin cred secret from vault/secret manager
    postgresAdminUsernameSecret:
    postgresAdminPasswordSecret:

    postgresAdminCredsSecretName: # replace with postgres admin secret
    postgresDBHost:  # replace with postgres endpoint DNS url

    pgCACertSourceSecretName:

    ## postgresql image
    postgresqlImage:
      repository: docker-registry-proxy.corp.amdocs.com/bitnami/postgresql
      tag: 15.3.0-debian-11-r12
      sha: ""

    # Master DB
    postgresqlDefaultDbName: postgres

  ## Deploy default dashboards.
  ##
  defaultDashboardsEnabled: true
  plugins:
    - http://illin4261.corp.amdocs.com:28080/repository/PLATFORM_FOSS_RAW/com/amdocs/platform/grafana/plugins/camptocamp-prometheus-alertmanager-datasource-1.2.1.zip;camptocamp-prometheus-alertmanager-datasource
    - http://illin4261.corp.amdocs.com:28080/repository/PLATFORM_FOSS_RAW/com/amdocs/platform/grafana/plugins/grafana-polystat-panel-2.0.4.zip;grafana-polystat-panel
  grafana.ini:
    ## LDAP Authentication can be enabled with the following values on grafana.ini
    ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid
    plugins:
      allow_loading_unsigned_plugins:
    analytics:
      check_for_updates: false
      check_for_plugin_updates: false
    quota:
      enabled: true
      global_session: 1
    auth:
      login_maximum_inactive_lifetime_duration: 30m  # Kills session after 30 mins of inactivity
    auth.ldap:
      enabled: false
      allow_sign_up: false
      config_file: /etc/grafana/ldap.toml
  ldap:
    enabled: false
    # `existingSecret` is a reference to an existing secret containing the ldap configuration
    # for Grafana in a key `ldap-toml`.
    existingSecret: ""
    # `config` is the content of `ldap.toml` that will be stored in the created secret

    config: |-
      # To troubleshoot and get the log information, enable LDAP debug logging in grafana.ini
      #[log]
      #filters = ldap:debug

      # To log user information returned from LDAP, set the value to true
      verbose_logging = true

      [[servers]]
      # LDAP server host (specify multiple hosts space separated)
      host = "my-server"

      # Default port is `389` or `636` if use_ssl = true
      port = 389

      # Search user bind dn
      bind_dn = "CN=admin,DC=grafana,DC=org"

      # Search user bind password
      # If the password contains # or ; you have to wrap it with triple quotes. Ex """#password;"""
      bind_password = 'grafana'

      # User search filter, for example "(CN=%s)" or "(sAMAccountName=%s)" or "(uid=%s)"
      search_filter = "(sAMAccountName=%s)"

      # An array of base dns to search through
      search_base_dns = ["DC=grafana,DC=org"]
      # In POSIX LDAP schemas, without memberOf attribute a secondary query must be made for groups.
      # This is done by enabling group_search_filter below. You must also set member_of= "CN"
      # in [servers.attributes] below.

      # Users with nested/recursive group membership and an LDAP server that supports LDAP_MATCHING_RULE_IN_CHAIN
      # can set group_search_filter, group_search_filter_user_attribute, group_search_base_dns and member_of
      # below in such a way that the user's recursive group membership is considered.
      #
      # Nested Groups + Active Directory (AD) Example:
      #
      #   AD groups store the Distinguished Names (DNs) of members, so your filter must
      #   recursively search your groups for the authenticating user's DN. For example:
      #
      #     group_search_filter = "(member:1.2.840.113556.1.4.1941:=%s)"
      #     group_search_filter_user_attribute = "distinguishedName"
      #     group_search_base_dns = ["ou=groups,DC=grafana,DC=org"]
      #
      #     [servers.attributes]
      #     ...
      #     member_of = "distinguishedName"

      ## Group search filter, to retrieve the groups of which the user is a member (only set if memberOf attribute is not available)
      # group_search_filter = "(&(objectClass=posixGroup)(memberUid=%s))"
      ## Group search filter user attribute defines what user attribute gets substituted for %s in group_search_filter.
      ## Defaults to the value of username in [server.attributes]
      ## Valid options are any of your values in [servers.attributes]
      ## If you are using nested groups you probably want to set this and member_of in
      ## [servers.attributes] to "distinguishedName"
      # group_search_filter_user_attribute = "distinguishedName"
      ## An array of the base DNs to search through for groups. Typically uses ou=groups
      group_search_base_dns = ["ou=groups,DC=grafana,DC=org"]
      # Specify names of the LDAP attributes your LDAP uses
      [servers.attributes]
      name = "givenName"
      surname = "sn"
      username = "sAMAccountName"
      member_of = "memberOf"
      email =  "mail"

      # Map LDAP groups to grafana org roles

      #[[servers.group_mappings]]
      #group_dn = "CN=superadmins,DC=grafana,DC=org"
      #group_dn = "CN=ldap,OU=People,DC=test,DC=example,DC=com"
      #org_role = "Admin"

      #[[servers.group_mappings]]
      #group_dn = "CN=admins,DC=grafana,DC=org"
      #org_role = "Admin"

      #[[servers.group_mappings]]
      #group_dn = "CN=users,DC=grafana,DC=org"
      #org_role = "Editor"

      #[[servers.group_mappings]]
      #group_dn = "*"
      #org_role = "Viewer"

  ingress:
    ## If true, Grafana Ingress will be created
    ##
    enabled: false

    ## Annotations for Grafana Ingress
    ##
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"

    ## Labels to be added to the Ingress
    ##
    labels: {}

    ## Hostnames.
    ## Must be provided if Ingress is enable.
    ##
    # hosts:
    #   - grafana.domain.com
    hosts: []

    ## Path for grafana ingress
    path: /

    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    ##
    tls: []
    # - secretName: grafana-general-tls
    #   hosts:
    #   - grafana.example.com
  initChownData:
    image:
      repository: docker-registry-proxy.corp.amdocs.com/busybox
      tag: "1.34.1"
      sha: ""
      pullPolicy: IfNotPresent
  downloadDashboardsImage:
    repository: docker.io/curlimages/curl
    tag: latest
    sha: ""
    pullPolicy: IfNotPresent
  sidecar:
    image:
      repository: docker-registry-proxy.corp.amdocs.com/kiwigrid/k8s-sidecar
      tag: 1.24.3
      sha: ""
    dashboards:
      enabled: true
      label: grafana_dashboard

      ## Annotations for Grafana dashboard configmaps
      ##
      annotations: {}
      multicluster: false
    datasources:
      enabled: true
      defaultDatasourceEnabled: true

      ## URL of prometheus datasource
      ##
      # url: http://prometheus-stack-prometheus:9090/

      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default
      # defaultDatasourceScrapeInterval: 15s

      ## Annotations for Grafana datasource configmaps
      ##
      annotations: {}

      ## Create datasource for each Pod of Prometheus StatefulSet;
      ## this uses headless service `prometheus-operated` which is
      ## created by Prometheus Operator
      ## ref: https://git.io/fjaBS
      createPrometheusReplicasDatasources: false
      label: grafana_datasource

  extraConfigmapMounts: []
  # - name: certs-configmap
  #   mountPath: /etc/grafana/ssl/
  #   configMap: certs-configmap
  #   readOnly: true

  ## Configure additional grafana datasources (passed through tpl)
  ## ref: http://docs.grafana.org/administration/provisioning/#datasources
  additionalDataSources: []
  # - name: prometheus-sample
  #   access: proxy
  #   basicAuth: true
  #   basicAuthPassword: pass
  #   basicAuthUser: daco
  #   editable: false
  #   jsonData:
  #       tlsSkipVerify: true
  #   orgId: 1
  #   type: prometheus
  #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
  #   version: 1

  ## Passed to grafana subchart and used by servicemonitor below
  ##
  service:
    portName: service

  ## If true, create a serviceMonitor for grafana
  ##
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    selfMonitor: true

    # Path to use for scraping metrics. Might be different if server.root_url is set
    # in grafana.ini
    path: "/metrics"

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

## Component scraping the kube api server
##
kubeApiServer:
  enabled: true
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: false
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    jobLabel: component
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    relabelings: []
    # - sourceLabels:
    #     - __meta_kubernetes_namespace
    #     - __meta_kubernetes_service_name
    #     - __meta_kubernetes_endpoint_port_name
    #   action: keep
    #   regex: default;kubernetes;https
    # - targetLabel: __address__
    #   replacement: kubernetes.default.svc:443

## Component scraping the kubelet and kubelet-hosted cAdvisor
##
kubelet:
  enabled: true
  namespace: kube-system

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## Enable scraping the kubelet over https. For requirements to enable this see
    ## https://github.com/prometheus-operator/prometheus-operator/issues/926
    ##
    https: true

    ## Enable scraping /metrics/cadvisor from kubelet's service
    ##
    cAdvisor: true

    ## Enable scraping /metrics/probes from kubelet's service
    ##
    probes: true

    ## Enable scraping /metrics/resource from kubelet's service
    ## This is disabled by default because container metrics are already exposed by cAdvisor
    ##
    resource: false
    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
    resourcePath: "/metrics/resource/v1alpha1"
    ## Metric relabellings to apply to samples before ingestion
    ##
    cAdvisorMetricRelabelings: []
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop

    ## Metric relabellings to apply to samples before ingestion
    ##
    probesMetricRelabelings: []
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop

    # relabel configs to apply to samples before ingestion.
    #   metrics_path is required to match upstream rules and charts
    ##
    cAdvisorRelabelings:
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    probesRelabelings:
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    resourceRelabelings:
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    metricRelabelings: []
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop

    # relabel configs to apply to samples before ingestion.
    #   metrics_path is required to match upstream rules and charts
    ##
    relabelings:
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

## Component scraping the kube controller manager
##
kubeControllerManager:
  enabled: true

  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## If using kubeControllerManager.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    port: 10252
    targetPort: 10252
    # selector:
    #   component: kube-controller-manager

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## Enable scraping kube-controller-manager over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    ##
    https: false

    # Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    # Name of the server to use when validating TLS certificate
    serverName: null

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
  enabled: true
  service:
    port: 9153
    targetPort: 9153
    # selector:
    #   k8s-app: kube-dns
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

## Component scraping kubeDns. Use either this or coreDns
##
kubeDns:
  enabled: false
  service:
    dnsmasq:
      port: 10054
      targetPort: 10054
    skydns:
      port: 10055
      targetPort: 10055
    # selector:
    #   k8s-app: kube-dns
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    dnsmasqMetricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    dnsmasqRelabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

## Component scraping etcd
##
kubeEtcd:
  enabled: true

  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    port: 2379
    targetPort: 2379
    # selector:
    #   component: etcd

  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
  ## specifying security configuration below. For example, with a secret named etcd-client-cert
  ##
  ## serviceMonitor:
  ##   scheme: https
  ##   insecureSkipVerify: false
  ##   serverName: localhost
  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  ##
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""
    scheme: http
    insecureSkipVerify: false
    serverName: ""
    caFile: ""
    certFile: ""
    keyFile: ""

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace


## Component scraping kube scheduler
##
kubeScheduler:
  enabled: true

  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  ## If using kubeScheduler.endpoints only the port and targetPort are used
  ##
  service:
    enabled: true
    port: 10251
    targetPort: 10251
    # selector:
    #   component: kube-scheduler

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""
    ## Enable scraping kube-scheduler over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    ##
    https: false

    ## Skip TLS certificate validation when scraping
    insecureSkipVerify: null

    ## Name of the server to use when validating TLS certificate
    serverName: null

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace


## Component scraping kube proxy
##
kubeProxy:
  enabled: true

  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
  ##
  endpoints: []
  # - 10.141.4.22
  # - 10.141.4.23
  # - 10.141.4.24

  service:
    enabled: true
    port: 10249
    targetPort: 10249
    # selector:
    #   k8s-app: kube-proxy

  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## Enable scraping kube-proxy over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
    ##
    https: false

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]


## Component scraping kube state metrics
##
kubeStateMetrics:
  enabled: true
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""
    ## Override serviceMonitor selector
    ##
    selectorOverride: {}
    ## Override namespace selector
    ##
    namespaceOverride: ""

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    # Enable self metrics configuration for Service Monitor
    selfMonitor:
      enabled: false

## Configuration for kube-state-metrics subchart
##
kube-state-metrics:
  enabled: true
  namespaceOverride: ""
  rbac:
    create: true
  image:
    repository: k8s.gcr.io/kube-state-metrics/kube-state-metrics
    tag: v2.9.2
  imagePullSecrets: []
  # - name: "image-pull-secret"


## Deploy node exporter as a daemonset to all nodes
##
nodeExporter:
  enabled: true

  ## Use the value configured in prometheus-node-exporter.podLabels
  ##
  jobLabel: jobLabel

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""

    ## proxyUrl: URL of a proxy that should be used for scraping.
    ##
    proxyUrl: ""

    ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
    ##
    scrapeTimeout: ""

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
    #   replacement: $1
    #   action: drop

    ## relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

## Configuration for prometheus-node-exporter subchart
##
prometheus-node-exporter:
  enabled: true
  namespaceOverride: ""
  podLabels:
    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
    ##
    jobLabel: node-exporter
  extraArgs:
    - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  image:
    repository: docker-registry-proxy.corp.amdocs.com/prometheus/node_exporter
    tag: v1.6.0

prometheus-cloudwatch-exporter:
  enabled: true
  image:
    repository: docker-registry-proxy.corp.amdocs.com/prom/cloudwatch-exporter
    tag: v0.15.1

prometheus-pushgateway:
  enabled: true
  image:
    repository: docker-registry-proxy.corp.amdocs.com/pushgateway/pushgateway
    tag: v1.6.0
  imagePullSecrets: []
  #  - name: "nexus-illin4261-6000-pull-secret"
  serviceMonitor:
    enabled: true

promitor-agent-scraper:
  enabled: false
  secrets:
    createSecret: false
    secretName: "promitor-secret"
    appIdSecret: username
    appKeySecret: password
  metricSinks:
    prometheusScrapingEndpoint:
      serviceMonitor:
        enabled: true
        labels:
          monitored: dox-prometheus
        interval: 60s
        timeout: 10s
  image:
    repository: docker-registry-proxy.corp.amdocs.com/ghcr.io/tomkerkhove/promitor-agent-scraper
    tag: 2.9.1
  azureMetadata:
    tenantId: "AKS_AZURE_TENANT_ID"
    subscriptionId: "AKS_AZURE_SUBSCRIPTION_ID"
  metricDefaults:
    aggregation:
      interval: 00:02:00
    scraping:
      schedule: "0 */2 * ? * *"
  metrics:
    - name: PostgresCpu
      description: "Postgres % Cpu"
      resourceType: Generic
      azureMetricConfiguration:
        metricName: cpu_percent
        aggregation:
          type: Average
      resources:
      - resourceUri: Microsoft.DBforPostgreSQL/flexibleServers/postgres-pbgms-nts1
        resourceGroupName: rg_pg_pbgms360-nts-01
    - name: PostgresMemory
      description: "Memory %"
      resourceType: Generic
      azureMetricConfiguration:
        metricName: memory_percent
        aggregation:
          type: Average
      resources:
      - resourceUri: Microsoft.DBforPostgreSQL/flexibleServers/postgres-pbgms-nts1
        resourceGroupName: rg_pg_pbgms360-nts-01
    - name: PostgresIops
      description: "Iops"
      resourceType: Generic
      azureMetricConfiguration:
        metricName: iops
        aggregation:
          type: Average
      resources:
      - resourceUri: Microsoft.DBforPostgreSQL/flexibleServers/postgres-pbgms-nts1
        resourceGroupName: rg_pg_pbgms360-nts-01
    - name: azure_kubernetes_available_cpu_cores
      description: "Available CPU cores in cluster"
      resourceType: KubernetesService
      azureMetricConfiguration:
        metricName: kube_node_status_allocatable_cpu_cores
        aggregation:
          type: Average
      resources:
      - clusterName: "TOKEN_AKS_CLUSTER_NAME"
        resourceGroupName: "AKS_AZURE_RESOURCE_GROUP"

## Manages Prometheus and Alertmanager components
##
prometheusOperator:
  enabled: true

  ## Prometheus-Operator v0.39.0 and later support TLS natively.
  ##
  tls:
    enabled: false
    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
    tlsMinVersion: VersionTLS13
    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
    internalPort: 10250

  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
  ## rules from making their way into prometheus and potentially preventing the container from starting
  admissionWebhooks:
    failurePolicy: Fail
    enabled: false
    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
    ## If unspecified, system trust roots on the apiserver are used.
    caBundle: ""
    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
    ## certs ahead of time if you wish.
    ##
    patch:
      enabled: false
      image:
        repository: jettech/kube-webhook-certgen
        tag: v1.5.2
        sha: ""
        pullPolicy: IfNotPresent
      resources: {}
      # limits:
      #   cpu: 250m
      #   memory: 1Gi
      # requests:
      #   cpu: 50m
      #   memory: 1Gi
      ## Provide a priority class name to the webhook patching job
      ##
      priorityClassName: ""
      podAnnotations: {}
      nodeSelector: {}
      affinity: {}
      tolerations: []

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 2000 and gid 2000.*v1.PodSecurityContextfalse
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000

    # Use certmanager to generate webhook certs
    certManager:
      enabled: false
      # issuerRef:
      #   name: "issuer"
      #   kind: "ClusterIssuer"

  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
  ##
  namespaces: {}
    # releaseNamespace: true
    # additional:
    # - kube-system

  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
  ##
  denyNamespaces: []

  ## Filter namespaces to look for prometheus-operator custom resources
  ##
  alertmanagerInstanceNamespaces: []
  prometheusInstanceNamespaces: []
  thanosRulerInstanceNamespaces: []

  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
  ##
  # clusterDomain: "cluster.local"

  ## Service account for Alertmanager to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    create: true
    name: ""

  ## Configuration for Prometheus operator service
  ##
  service:
    annotations: {}
    labels: {}
    clusterIP: ""

  ## Port to expose on each node
  ## Only used if service.type is 'NodePort'
  ##
    nodePort: 30080

    nodePortTls: 30443

  ## Additional ports to open for Prometheus service
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
  ##
    additionalPorts: []

  ## Loadbalancer IP
  ## Only use if service.type is "LoadBalancer"
  ##
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  ## Service type
  ## NodePort, ClusterIP, LoadBalancer
  ##
    type: ClusterIP

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

  ## Labels to add to the operator pod
  ##
  podLabels: {}

  ## Annotations to add to the operator pod
  ##
  podAnnotations: {}

  ## Assign a PriorityClassName to pods if set
  # priorityClassName: ""

  ## Define Log Format
  # Use logfmt (default) or json logging
  # logFormat: logfmt

  ## Decrease log verbosity to errors only
  # logLevel: error

  ## If true, the operator will create and maintain a service for scraping kubelets
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/helm/prometheus-operator/README.md
  ##
  kubeletService:
    enabled: true
    namespace: kube-system

  ## Create a servicemonitor for the operator
  ##
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.
    scrapeTimeout: ""
    selfMonitor: true

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

  ## Resource limits & requests
  ##
  resources: {}
  # limits:
  #   cpu: 200m
  #   memory: 200Mi
  # requests:
  #   cpu: 100m
  #   memory: 100Mi

  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
  ##
  hostNetwork: false

  ## Define which Nodes the Pods are scheduled on.
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Tolerations for use with node taints
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"

  ## Assign custom affinity rules to the prometheus operator
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: kubernetes.io/e2e-az-name
    #         operator: In
    #         values:
    #         - e2e-az1
    #         - e2e-az2
  dnsConfig: {}
    # nameservers:
    #   - 1.2.3.4
    # searches:
    #   - ns1.svc.cluster-domain.example
    #   - my.dns.search.suffix
    # options:
    #   - name: ndots
    #     value: "2"
  #   - name: edns0
  # securityContext:
  #   fsGroup: 65534
  #   runAsGroup: 65534
  #   runAsNonRoot: true
  #   runAsUser: 65534

  ## Prometheus-operator image
  ##
  image:
    repository: docker-registry-proxy.corp.amdocs.com/prometheus-operator/prometheus-operator
    tag: v0.65.2
    sha: ""
    pullPolicy: IfNotPresent

  ## Prometheus image to use for prometheuses managed by the operator
  ##
  # prometheusDefaultBaseImage: quay.io/prometheus/prometheus

  ## Alertmanager image to use for alertmanagers managed by the operator
  ##
  # alertmanagerDefaultBaseImage: quay.io/prometheus/alertmanager

  ## Prometheus-config-reloader image to use for config and rule reloading
  ##
  prometheusConfigReloaderImage:
    repository: docker-registry-proxy.corp.amdocs.com/prometheus-operator/prometheus-config-reloader
    tag: v0.65.2
    sha: ""

    # resource config for prometheusConfigReloader
    resources:
      requests:
        cpu: 100m
        memory: 50Mi
      limits:
        cpu: 100m
        memory: 50Mi

  ## Thanos side-car image when configured
  ##
  thanosImage:
    repository: quay.io/thanos/thanos
    tag: v0.17.2
    sha: ""

  ## Set a Field Selector to filter watched secrets
  ##
  secretFieldSelector: ""

## Deploy a Prometheus instance
##
prometheus:



  enabled: true

  ## Annotations for Prometheus
  ##
  annotations: {}

  ## Service account for Prometheuses to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    create: true
    name: ""
    annotations: {}

  # Service for thanos service discovery on sidecar
  # Enable this can make Thanos Query can use
  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery
  # Thanos sidecar on prometheus nodes
  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
  thanosService:
    enabled: false
    annotations: {}
    labels: {}
    portName: grpc
    port: 10901
    targetPort: "grpc"
    clusterIP: "None"

    ## Service type
    ##
    type: ClusterIP

    ## Port to expose on each node
    ##
    nodePort: 30901

  # Service for external access to sidecar
  # Enabling this creates a service to expose thanos-sidecar outside the cluster.
  thanosServiceExternal:
    enabled: false
    annotations: {}
    labels: {}
    portName: grpc
    port: 10901
    targetPort: "grpc"

    ## Service type
    ##
    type: LoadBalancer

    ## Port to expose on each node
    ##
    nodePort: 30901

  ## Configuration for Prometheus service
  ##
  service:
    annotations: {}
    labels:
      app.kubernetes.io/part-of: infra-platform
      app.kubernetes.io/component: prometheus
      app.amdocs.com/domain: monitoring
      app.amdocs.com/provider: amdocs

    clusterIP: ""

    ## Port for Prometheus Service to listen on
    ##
    port: 9090

    ## To be used with a proxy extraContainer port
    targetPort: 9090

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
    nodePort: 30090

    ## Loadbalancer IP
    ## Only use if service.type is "LoadBalancer"
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    ## Service type
    ##
    type: ClusterIP

    sessionAffinity: ""

  ## Configuration for creating a separate Service for each statefulset Prometheus replica
  ##
  servicePerReplica:
    enabled: false
    annotations: {}

    ## Port for Prometheus Service per replica to listen on
    ##
    port: 9090

    ## To be used with a proxy extraContainer port
    targetPort: 9090

    ## Port to expose on each node
    ## Only used if servicePerReplica.type is 'NodePort'
    ##
    nodePort: 30091

    ## Loadbalancer source IP ranges
    ## Only used if servicePerReplica.type is "LoadBalancer"
    loadBalancerSourceRanges: []
    ## Service type
    ##
    type: ClusterIP

  ## Configure pod disruption budgets for Prometheus
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  ## This configuration is immutable once created and will require the PDB to be deleted to be changed
  ## https://github.com/kubernetes/kubernetes/issues/45398
  ##
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""

  # Ingress exposes thanos sidecar outside the cluster
  thanosIngress:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}
    labels: {}
    servicePort: 10901

    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
    nodePort: 30901

    ## Hosts must be provided if Ingress is enabled.
    ##
    hosts: []
      # - thanos-gateway.domain.com

    ## Paths to use for ingress rules
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## TLS configuration for Thanos Ingress
    ## Secret must be manually created in the namespace
    ##
    tls: []
    # - secretName: thanos-gateway-tls
    #   hosts:
    #   - thanos-gateway.domain.com

  ingress:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}
    labels: {}

    ## Hostnames.
    ## Must be provided if Ingress is enabled.
    ##
    # hosts:
    #   - prometheus.domain.com
    hosts: []

    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## TLS configuration for Prometheus Ingress
    ## Secret must be manually created in the namespace
    ##
    tls: []
      # - secretName: prometheus-general-tls
      #   hosts:
      #     - prometheus.example.com

  ## Configuration for creating an Ingress that will map to each Prometheus replica service
  ## prometheus.servicePerReplica must be enabled
  ##
  ingressPerReplica:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}
    labels: {}

    ## Final form of the hostname for each per replica ingress is
    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
    ##
    ## Prefix for the per replica ingress that will have `-$replicaNumber`
    ## appended to the end
    hostPrefix: ""
    ## Domain that will be used for the per replica ingress
    hostDomain: ""

    ## Paths to use for ingress rules
    ##
    paths: []
    # - /

    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific

    ## Secret name containing the TLS certificate for Prometheus per replica ingress
    ## Secret must be manually created in the namespace
    tlsSecretName: ""

    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
    ##
    tlsSecretPerReplica:
      enabled: false
      ## Final form of the secret for each per replica ingress is
      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
      ##
      prefix: "prometheus"

  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    ##
    interval: ""
    selfMonitor: true

    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""

    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
    tlsConfig: {}

    bearerTokenFile:

    ## metric relabel configs to apply to samples before ingestion.
    ##
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # relabel configs to apply to samples before ingestion.
    ##
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

  ## Settings affecting prometheusSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  ##
  prometheusSpec:
    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
    ##
    disableCompaction: false
    ## APIServerConfig
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#apiserverconfig
    ##
    apiserverConfig: {}

    ## Interval between consecutive scrapes.
    ## Defaults to 30s.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
    ##
    scrapeInterval: ""

    ## Number of seconds to wait for target to respond before erroring
    ##
    scrapeTimeout: ""

    ## Interval between consecutive evaluations.
    ##
    evaluationInterval: ""

    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
    ##
    listenLocal: false

    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
    ## This is disabled by default.
    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
    ##
    enableAdminAPI: false

    # EnableFeatures API enables access to Prometheus disabled features.
    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/
    enableFeatures: []
    # - exemplar-storage

    ## Image of Prometheus.
    ##
    image:
      repository: docker-registry-proxy.corp.amdocs.com/prometheus/prometheus
      tag: v2.44.0
      sha: ""

    ## Tolerations for use with node taints
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []
    #  - key: "key"
    #    operator: "Equal"
    #    value: "value"
    #    effect: "NoSchedule"

    ## If specified, the pod's topology spread constraints.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    ##
    topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app: prometheus

    ## Alertmanagers to which alerts will be sent
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints
    ##
    ## Default configuration will connect to the alertmanager deployed as part of this release
    ##
    alertingEndpoints: []
    # - name: ""
    #   namespace: ""
    #   port: http
    #   scheme: http
    #   pathPrefix: ""
    #   tlsConfig: {}
    #   bearerTokenFile: ""
    #   apiVersion: v2

    ## External labels to add to any time series or alerts when communicating with external systems
    ##
    externalLabels: {}

    ## Name of the external label used to denote replica name
    ##
    replicaExternalLabelName: ""

    ## If true, the Operator won't add the external label used to denote replica name
    ##
    replicaExternalLabelNameClear: false

    ## Name of the external label used to denote Prometheus instance name
    ##
    prometheusExternalLabelName: ""

    ## If true, the Operator won't add the external label used to denote Prometheus instance name
    ##
    prometheusExternalLabelNameClear: false

    ## External URL at which Prometheus will be reachable.
    ##
    externalUrl: ""

    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    ##
    nodeSelector: {}

    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
    ## with the new list of secrets.
    ##
    secrets: []

    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
    ##
    configMaps: []

    ## QuerySpec defines the query command line flags when starting Prometheus.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#queryspec
    ##
    query: {}

    ## Namespaces to be selected for PrometheusRules discovery.
    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
    ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
    ##
    ruleNamespaceSelector: {}

    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the PrometheusRule resources created
    ##
    ruleSelectorNilUsesHelmValues: false

    ## PrometheusRules to be selected for target discovery.
    ## If {}, select all PrometheusRules
    ##
    ruleSelector:
      matchLabels:
        monitored: dox-prometheus

    ## Example which select all PrometheusRules resources
    ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
    # ruleSelector:
    #   matchExpressions:
    #     - key: prometheus
    #       operator: In
    #       values:
    #         - example-rules
    #         - example-rules-2
    #
    ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
    # ruleSelector:
    #   matchLabels:
    #     role: example-rules

    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    ##
    serviceMonitorSelectorNilUsesHelmValues: true

    ## ServiceMonitors to be selected for target discovery.
    ## If {}, select all ServiceMonitors
    ##
    serviceMonitorSelector:
      matchLabels:
        monitored: dox-prometheus

    ## Namespaces to be selected for ServiceMonitor discovery.
    ##
    serviceMonitorNamespaceSelector: {}
    ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
    # serviceMonitorNamespaceSelector:
    #   matchLabels:
    #     prometheus: somelabel

    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the podmonitors created
    ##
    podMonitorSelectorNilUsesHelmValues: false

    ## PodMonitors to be selected for target discovery.
    ## If {}, select all PodMonitors
    ##
    podMonitorSelector:
      matchLabels:
        monitored: dox-prometheus


    ## Namespaces to be selected for PodMonitor discovery.
    ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
    ##
    podMonitorNamespaceSelector: {}

    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the probes created
    ##
    probeSelectorNilUsesHelmValues: false

    ## Probes to be selected for target discovery.
    ## If {}, select all Probes
    ##
    probeSelector:
      matchLabels:
        monitored: dox-prometheus


    ## Namespaces to be selected for Probe discovery.
    ## See https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
    ##
    probeNamespaceSelector: {}

    ## How long to retain metrics
    ##
    retention: 10d

    ## Maximum size of metrics
    ##
    retentionSize: ""

    ## Enable compression of the write-ahead log using Snappy.
    ##
    walCompression: false

    ## If true, the Operator won't process any Prometheus configuration changes
    ##
    paused: false

    ## Number of replicas of each shard to deploy for a Prometheus deployment.
    ## Number of replicas multiplied by shards is the total number of Pods created.
    ##
    replicas: 1

    ## EXPERIMENTAL: Number of shards to distribute targets onto.
    ## Number of replicas multiplied by shards is the total number of Pods created.
    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.
    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.
    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.
    ## Sharding is done on the content of the `__address__` target meta-label.
    ##
    shards: 1

    ## Log level for Prometheus be configured in
    ##
    logLevel: info

    ## Log format for Prometheus be configured in
    ##
    logFormat: logfmt

    ## Prefix used to register routes, overriding externalUrl route.
    ## Useful for proxies that rewrite URLs.
    ##
    routePrefix: /

    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
    ## Metadata Labels and Annotations gets propagated to the prometheus pods.
    ##
    podMetadata:
      labels:
        app.kubernetes.io/part-of: infra-platform
        app.kubernetes.io/component: prometheus
        app.amdocs.com/domain: monitoring
        app.amdocs.com/provider: amdocs
    #   app: prometheus
    #   k8s-app: prometheus

    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
    ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
    ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
    ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
    podAntiAffinity: "soft"

    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
    ##
    podAntiAffinityTopologyKey: kubernetes.io/hostname

    ## Assign custom affinity rules to the prometheus instance
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    ##
    affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: kubernetes.io/e2e-az-name
    #         operator: In
    #         values:
    #         - e2e-az1
    #         - e2e-az2

    ## The remote_read spec configuration for Prometheus.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
    remoteRead: []
    # - url: http://remote1/read
    ## additionalRemoteRead is appended to remoteRead
    additionalRemoteRead: []

    ## The remote_write spec configuration for Prometheus.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
    remoteWrite: []
    # - url: http://remote1/push
    ## additionalRemoteWrite is appended to remoteWrite
    additionalRemoteWrite: []

    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
    remoteWriteDashboards: false

    ## Resource limits & requests
    ##
    resources: {}
    # limits:
    #   cpu: 250m
    #   memory: 1Gi
    # requests:
    #   cpu: 50m
    #   memory: 1Gi

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
    storageSpec: {}
    ## Using PersistentVolumeClaim
    ##
    #  volumeClaimTemplate:
    #    spec:
    #      storageClassName: gluster
    #      accessModes: ["ReadWriteOnce"]
    #      resources:
    #        requests:
    #          storage: 50Gi
    #    selector: {}

    ## Using tmpfs volume
    ##
    #  emptyDir:
    #    medium: Memory

    # Additional volumes on the output StatefulSet definition.
    volumes: []

    # Additional VolumeMounts on the output StatefulSet definition.
    volumeMounts: []

    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
    ## as specified in the official Prometheus documentation:
    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
    ## scrape configs are going to break Prometheus after the upgrade.
    ##
    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
    ##
    additionalScrapeConfigs: []
    # - job_name: kube-etcd
    #   kubernetes_sd_configs:
    #     - role: node
    #   scheme: https
    #   tls_config:
    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    #   relabel_configs:
    #   - action: labelmap
    #     regex: __meta_kubernetes_node_label_(.+)
    #   - source_labels: [__address__]
    #     action: replace
    #     targetLabel: __address__
    #     regex: ([^:;]+):(\d+)
    #     replacement: ${1}:2379
    #   - source_labels: [__meta_kubernetes_node_name]
    #     action: keep
    #     regex: .*mst.*
    #   - source_labels: [__meta_kubernetes_node_name]
    #     action: replace
    #     targetLabel: node
    #     regex: (.*)
    #     replacement: ${1}
    #   metric_relabel_configs:
    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
    #     action: labeldrop

    ## If additional scrape configurations are already deployed in a single secret file you can use this section.
    ## Expected values are the secret name and key
    ## Cannot be used with additionalScrapeConfigs
    additionalScrapeConfigsSecret: {}
      # enabled: false
      # name:
      # key:

    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
    additionalPrometheusSecretsAnnotations: {}

    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
    ##
    additionalAlertManagerConfigs: []
    # - consul_sd_configs:
    #   - server: consul.dev.test:8500
    #     scheme: http
    #     datacenter: dev
    #     tag_separator: ','
    #     services:
    #       - metrics-prometheus-alertmanager

    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
    ## them separately from the helm deployment, you can use this section.
    ## Expected values are the secret name and key
    ## Cannot be used with additionalAlertManagerConfigs
    additionalAlertManagerConfigsSecret: {}
      # name:
      # key:

    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
    ## configs are going to break Prometheus after the upgrade.
    ##
    additionalAlertRelabelConfigs: []
    # - separator: ;
    #   regex: prometheus_replica
    #   replacement: $1
    #   action: labeldrop

    ## SecurityContext holds pod-level security attributes and common container settings.
    ## This defaults to non root user with uid 1000 and gid 2000.
    ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md
    ##
    # securityContext:
    #   runAsGroup: 2000
    #   runAsNonRoot: true
    #   runAsUser: 1000
    #   fsGroup: 2000

    ## Priority class assigned to the Pods
    ##
    priorityClassName: ""

    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.
    ## This section is experimental, it may change significantly without deprecation notice in any release.
    ## This is experimental and may change significantly without backward compatibility in any release.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#thanosspec
    ##
    thanos: {}

    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.
    ##  if using proxy extraContainer  update targetPort with proxy container port
    containers: []

    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
    ## (permissions, dir tree) on mounted volumes before starting prometheus
    initContainers: []

    ## PortName to use for Prometheus.
    ##
    portName: "web"

    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files
    ## on the file system of the Prometheus container e.g. bearer token files.
    arbitraryFSAccessThroughSMs: false

    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor
    ## or PodMonitor to true, this overrides honor_labels to false.
    overrideHonorLabels: false

    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.
    overrideHonorTimestamps: false

    ## IgnoreNamespaceSelectors if set to true will ignore NamespaceSelector settings from the podmonitor and servicemonitor
    ## configs, and they will only discover endpoints within their current namespace. Defaults to false.
    ignoreNamespaceSelectors: false

    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.
    ## The label value will always be the namespace of the object that is being created.
    ## Disabled by default
    enforcedNamespaceLabel: ""

    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.
    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair
    prometheusRulesExcludedFromEnforce: []

    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,
    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such
    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions
    ## of Prometheus >= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)
    queryLogFile: false

    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit
    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall
    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.
    enforcedSampleLimit: false

    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental
    ## in Prometheus so it may change in any upcoming release.
    allowOverlappingBlocks: false

  additionalRulesForClusterRole: []
  #  - apiGroups: [ "" ]
  #    resources:
  #      - nodes/proxy
  #    verbs: [ "get", "list", "watch" ]

  additionalServiceMonitors: []
  ## Name of the ServiceMonitor to create
  ##
  # - name: ""

    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from
    ## the chart
    ##
    # additionalLabels: {}

    ## Service label for use in assembling a job name of the form <label value>-<port>
    ## If no label is specified, the service name is used.
    ##
    # jobLabel: ""

    ## labels to transfer from the kubernetes service to the target
    ##
    # targetLabels: []

    ## labels to transfer from the kubernetes pods to the target
    ##
    # podTargetLabels: []

    ## Label selector for services to which this ServiceMonitor applies
    ##
    # selector: {}

    ## Namespaces from which services are selected
    ##
    # namespaceSelector:
      ## Match any namespace
      ##
      # any: false

      ## Explicit list of namespace names to select
      ##
      # matchNames: []

    ## Endpoints of the selected service to be monitored
    ##
    # endpoints: []
      ## Name of the endpoint's service port
      ## Mutually exclusive with targetPort
      # - port: ""

      ## Name or number of the endpoint's target port
      ## Mutually exclusive with port
      # - targetPort: ""

      ## File containing bearer token to be used when scraping targets
      ##
      #   bearerTokenFile: ""

      ## Interval at which metrics should be scraped
      ##
      #   interval: 30s

      ## HTTP path to scrape for metrics
      ##
      #   path: /metrics

      ## HTTP scheme to use for scraping
      ##
      #   scheme: http

      ## TLS configuration to use when scraping the endpoint
      ##
      #   tlsConfig:

          ## Path to the CA file
          ##
          # caFile: ""

          ## Path to client certificate file
          ##
          # certFile: ""

          ## Skip certificate verification
          ##
          # insecureSkipVerify: false

          ## Path to client key file
          ##
          # keyFile: ""

          ## Server name used to verify host name
          ##
          # serverName: ""

  additionalPodMonitors: []
  ## Name of the PodMonitor to create
  ##
  # - name: ""

    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from
    ## the chart
    ##
    # additionalLabels: {}

    ## Pod label for use in assembling a job name of the form <label value>-<port>
    ## If no label is specified, the pod endpoint name is used.
    ##
    # jobLabel: ""

    ## Label selector for pods to which this PodMonitor applies
    ##
    # selector: {}

    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.
    ##
    # podTargetLabels: {}

    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    ##
    # sampleLimit: 0

    ## Namespaces from which pods are selected
    ##
    # namespaceSelector:
      ## Match any namespace
      ##
      # any: false

      ## Explicit list of namespace names to select
      ##
      # matchNames: []

    ## Endpoints of the selected pods to be monitored
    ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#podmetricsendpoint
    ##
    # podMetricsEndpoints: []
